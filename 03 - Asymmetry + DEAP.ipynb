{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asymmetry + DEAP (emotion)\n",
    "\n",
    "In this tutorial, we will be working on the DEAP dataset, a benchmark EEG emotion recognition dataset. \n",
    "\n",
    "In this dataset, there is a total of 32 participants, where each participant watches 40 1-minute videos.  Thus <code>s01.dat</code> is holding 40 batches.   For simplicity, we shall only use the first participant, i.e., <code>s01.dat</code>.   \n",
    "\n",
    "You can download the dataset by googling and ask permission from the owner by filling a form.\n",
    "\n",
    "Looking in each dat file (e.g., s01), it contains the data and label\n",
    "- Data ----- 40 x 40 x 8064 [\tvideo/batches x channel x samples ]\n",
    "- Label  ---- 40 x 4 \n",
    "\n",
    "Out of 40 channels, 32 channels were of EEG, and the rest of 8 of them from other sensors such as EOG (see the section 6.1 of the original paper).  We shall only extract the first 32 channels.   For the 8064, since the data is downsampled to 128Hz, thus one second contains around 128 samples, thus in one minute which is 60 seconds, it will be roughly 7680 samples.  The paper did not really talk a lot but it is likely there is  another 1.5 seconds before and after which total to 8064 samples (128 Hz * 63 seconds).\n",
    "\n",
    "The four labels correspond to valence, arousal, liking, and dominance, in this order.  We will only use valence and arousal, thus index 0 and 1 of the labels will be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "import mne\n",
    "from mne import create_info\n",
    "from mne.io import RawArray\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e55a7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5b98265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will set the plotting size\n",
    "mne.set_config('MNE_BROWSE_RAW_SIZE','10,5')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d447abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9f93d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the dataset\n",
    "\n",
    "Let's first create a simple dataset loader.   The code is explained using comments and is quite self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, path, stim):\n",
    "        _, _, filenames = next(os.walk(path))\n",
    "        filenames = sorted(filenames)\n",
    "        all_data = []\n",
    "        all_label = []\n",
    "        for dat in filenames:\n",
    "            temp = pickle.load(open(os.path.join(path,dat), 'rb'), encoding='latin1')\n",
    "            all_data.append(temp['data'])\n",
    "            \n",
    "            if stim == \"Valence\":\n",
    "                all_label.append(temp['labels'][:,:1])   #the first index is valence\n",
    "            elif stim == \"Arousal\":\n",
    "                all_label.append(temp['labels'][:,1:2]) # Arousal  #the second index is arousal\n",
    "                \n",
    "        self.data = np.vstack(all_data)   #shape: (40, 40, 8064) ==> data of 1 participant\n",
    "        self.label = np.vstack(all_label) #(40, )  ==> 1280 samples, each with a unique label (depend on the param \"stim\")\n",
    "        \n",
    "        del temp, all_data, all_label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        single_data  = self.data[idx]\n",
    "        single_label = (self.label[idx] > 5).astype(float)   #convert the scale to either 0 or 1 (to classification problem)\n",
    "        \n",
    "        batch = {\n",
    "            'data': torch.Tensor(single_data),\n",
    "            'label': torch.Tensor(single_label)\n",
    "        }\n",
    "        \n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"data/deap\"  #create a folder \"data\", and inside put s01.dat,....,s32.dat inside from the preprocessed folder from the DEAP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valence = Dataset(path, \"Valence\")\n",
    "dataset_arousal = Dataset(path, \"Arousal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try look at one sample using the index.  This is automatically mapped to the <code>__getitem__</code> function in the <code>Dataset</code> class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': tensor([[ 9.4823e-01,  1.6533e+00,  3.0137e+00,  ..., -2.8265e+00,\n",
       "          -4.4772e+00, -3.6769e+00],\n",
       "         [ 1.2471e-01,  1.3901e+00,  1.8351e+00,  ..., -2.9870e+00,\n",
       "          -6.2878e+00, -4.4743e+00],\n",
       "         [-2.2165e+00,  2.2920e+00,  2.7464e+00,  ..., -2.6371e+00,\n",
       "          -7.4065e+00, -6.7559e+00],\n",
       "         ...,\n",
       "         [ 2.3078e+02,  6.9672e+02,  1.1951e+03,  ...,  1.0108e+03,\n",
       "           1.2831e+03,  1.5200e+03],\n",
       "         [-1.5418e+03, -1.6180e+03, -1.6927e+03,  ..., -1.5784e+04,\n",
       "          -1.5782e+04, -1.5781e+04],\n",
       "         [ 6.3905e-03,  6.3905e-03,  6.3905e-03,  ..., -9.7608e-02,\n",
       "          -9.7608e-02, -9.7608e-02]]),\n",
       " 'label': tensor([1.])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_valence[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data:  torch.Size([40, 8064])\n",
      "Shape of label:  torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of data: \", dataset_valence[0]['data'].shape)  #40 channels of data, 8064 samples in 1 minute\n",
    "print(\"Shape of label: \", dataset_valence[0]['label'].shape) #just 1 single label; 0 or 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to look at our data and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset_valence[:]['data']\n",
    "label = dataset_valence[:]['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 40, 8064])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we got 40 trials (40 videos, each with 40 channels of data, each video contains 8064 EEG samples)\n",
    "data.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#so we got 40 labels, i.e., one label per video\n",
    "label.shape  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's count how many 0 and 1 in the valence dataset, to see if there is some imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels 1 in valence dataset:  19\n",
      "Labels 0 in valence dataset:  21\n"
     ]
    }
   ],
   "source": [
    "cond_1 = label == 1\n",
    "cond_0 = label == 0\n",
    "\n",
    "print(\"Labels 1 in valence dataset: \", len(label[cond_1]))\n",
    "print(\"Labels 0 in valence dataset: \", len(label[cond_0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also count in the arousal dataset, to see if there is some imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels 1 in arousal dataset:  19\n",
      "Labels 0 in arousal dataset:  21\n"
     ]
    }
   ],
   "source": [
    "cond_1 = label == 1\n",
    "cond_0 = label == 0\n",
    "\n",
    "print(\"Labels 1 in arousal dataset: \", len(label[cond_1]))\n",
    "print(\"Labels 0 in arousal dataset: \", len(label[cond_0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the first 32 channels are EEG and the rest of the 8 channels are other channels, let's check the median value of each channel to see whether there is a pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of 0 data: 0.0193\n",
      "Median of 1 data: 0.0043\n",
      "Median of 2 data: 0.0012\n",
      "Median of 3 data: 0.0000\n",
      "Median of 4 data: 0.0205\n",
      "Median of 5 data: 0.0323\n",
      "Median of 6 data: -0.0175\n",
      "Median of 7 data: 0.0104\n",
      "Median of 8 data: -0.0291\n",
      "Median of 9 data: -0.0003\n",
      "Median of 10 data: 0.0255\n",
      "Median of 11 data: -0.0350\n",
      "Median of 12 data: -0.0393\n",
      "Median of 13 data: -0.0604\n",
      "Median of 14 data: -0.0421\n",
      "Median of 15 data: -0.0049\n",
      "Median of 16 data: -0.0088\n",
      "Median of 17 data: 0.0713\n",
      "Median of 18 data: 0.0021\n",
      "Median of 19 data: 0.0022\n",
      "Median of 20 data: 0.0238\n",
      "Median of 21 data: 0.0390\n",
      "Median of 22 data: 0.0121\n",
      "Median of 23 data: 0.0137\n",
      "Median of 24 data: 0.0462\n",
      "Median of 25 data: 0.0395\n",
      "Median of 26 data: 0.0184\n",
      "Median of 27 data: 0.0176\n",
      "Median of 28 data: 0.0353\n",
      "Median of 29 data: -0.0102\n",
      "Median of 30 data: 0.0165\n",
      "Median of 31 data: -0.0148\n",
      "Median of 32 data: -25.4018\n",
      "Median of 33 data: 43.8558\n",
      "Median of 34 data: -3.7539\n",
      "Median of 35 data: 8.1745\n",
      "Median of 36 data: 6808.3848\n",
      "Median of 37 data: 48.6546\n",
      "Median of 38 data: 5085.2627\n",
      "Median of 39 data: -0.0009\n"
     ]
    }
   ],
   "source": [
    "for i in range(40):\n",
    "    print(f\"Median of {i} data: {torch.median(data[:, i, :]):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the data index 0 to 31 is clearly EEG, while data from 32 onward is not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c97dcdf",
   "metadata": {},
   "source": [
    "## 2. Artifact Removal\n",
    "\n",
    "Note that since the data is already preprocessed by the authors, we don't have to do anything more, but it's very natural for us to do preprocessing, e.g., min-max normalization, notch filters, band pass filters, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3facae",
   "metadata": {},
   "source": [
    "## 3. EDA - Spectral Analysis\n",
    "\n",
    "In this part 2, we will focus on feature engineering using spectral analysis.  Spectral analysis here refers to the analysis of theta (4 - 8 Hz), alpha (8 - 12 Hz), beta (12 - 30 Hz), and gamma (30 - 64 Hz).   \n",
    "\n",
    "Spectral analysis is a very basic and must-do analysis for emotions/cognitions/resting state since it is a common knowledge with abundant evidence that our emotion/cognition change how our brain signals oscillate.  For example, when we are calm, alpha is relatively high, likewise, when we are attentive, beta is relatively high and alpha becomes relatively lower.\n",
    "\n",
    "In this part, we shall extract these powers as features and then input them into SVM and see if these features are useful for predicting the four valence-arousal classes that we have obtained from Part 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81fe06c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 ('teaching_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "becc4c8e5ad229b2591d820334d85e3db0111492344629bf57f272470dce75a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
